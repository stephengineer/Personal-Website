# 简历

求职方向：大模型预训练、微调、Agent、RAG

## 教育背景

佐治亚理工学院(美国，佐治亚)
计算机科学(硕士)
2014-09 ~ 2016-06

加州大学洛杉矶分校(美国，洛杉矶)
数学(学士)
2010-09 ~ 2014-06

## 工作经验

---
海纳AI(中国，北京)
NLP算法负责人
2024-03 ~ 至今

离职原因：公司资金链断裂，无法支付员工薪资
实线管理：算法工程师3人

### AI面试官

跟随[InstructGPT](https://arxiv.org/abs/2203.02155)论文中数据类别、数量、长度分布利用私有数据源、Python爬虫技术，self-instruct方式，成功构建了高效、自动化的训练和测试数据pipeline生成了万条SFT数据。
采用LoRA和QLoRA技术对ChatGLM3/4、LLaMA2/3、Qwen、InternLM等不同参数规模的开源大模型进行SFT，确保模型输出的稳定性与一致性。
通过PPO、DPO和RLHF算法对pre-train模型输出进行了细致的对齐和优化，以适应面试领域的特定需求，实现了模型在面试领域的SOTA性能。
利用Embedding对比相关性改进了few-shot学习方式提升了小样本学习场景下的模型性能，结合CoT方法提出causal inference策略增强了模型对复杂问题解决的推理能力。
通过对比四种不同的模型优化方案，进行了细致的微调实验，最终实现了模型准确率的显著提升。
应用FastLLM、vLLM、llama.cpp、TensorRT等模型推理加速框架，结合Triton服务部署，显著提高了模型的吞吐量，达到了500 tokens/s。优化了模型的推理速度和资源利用率，确保了高并发场景下的稳定性。
使用Tornado服务将模型docker化部以Restful API形式部署到线上服务器，并使用OpenTelemetry埋点进行链路追踪监控模型推理过程，同时使用Crontab定时任务进行模型的健康监测并自动重启，确保了模型的稳定性和高可用性。

### 问答Agent系统（从0到1）

基于RAG（检索增强生成）架构开发智能问答agent，并成功集成了ElasticSearch中的Title/Subtitle/Content多域的全索引召回和排序，同时排序中加入可学习权重的语义相似度模型。
运用Scikit-learn库中的TF-IDF算法与Embedding检索和SimCSE算法优化了多路召回机制，显著提升问答精度与响应速度。确保了对复杂查询的即时响应和高度相关答案的精准检索。
提取用户query、doc、topic等特征训练LightGBM模型提高排序合理性。

### 系统架构设计

设计并实现了全面的智能面试系统架构，涵盖面试题生成、考核维度定义和综合评估等功能，支持个性化面试问题、自动追问与评分机制，提升线上面试的智能化和互动体验。
构建了基于业务需求的大模型测试和评估系统，实现了快速切换基座模型的pipeline，确保了面试测评的准确度和行业领先性。通过持续的模型评估和优化，保持了技术解决方案的竞争力。

---

目的涌现(中国，北京)
AI技术负责人
2023-07 ~ 2024-02
业务方向：AI+教育，通过3D虚拟世界的场景对话，结合社交与游戏玩法让用户练习口语。

### Multi-Agents对话引擎（从0到1）

基于In-Context-Learning（ICL）开发了[Stanford Village AGI](https://arxiv.org/abs/2304.03442)（斯坦福小镇）项目，负责从技术设计到项目部署的全生命周期，通过集成LangChain和Pinecone向量数据库，以及MySQL数据库，构建了RAG架构，显著提升了agent的长短期记忆和决策能力。
独立开发automatic agents与generative agent框架，结合CoT思路实现了multi-agent之间以及用户与agent自然对话的能力。
开发了一套高效的对话数据处理pipeline，引入多种文本处理技术管理对话内容与主题以配合RAG框架。制定了详细的数据标注规范，实现了数据的半自动labeling，确保了标注的一致性和高准确率，为模型训练提供了可靠的数据基础。
对比不同ASR和TTS技术并采用streaming接入产品，以及开源模型Whisper和DALLE-3本地化部署，构建agent聊天内容多样性，同时确保成本效益。
设计并实施了基于few-shot learning与prompt engineering的agent背景信息设定，通过GPT模型对千条对话语料进行distillation和SFT，精确控制agent情感表达，增强了交互的真实感。
使用FastAPI框架将agent项目部署到线上服务器，并根据QPS和RT等指标进行性能优化。

### 模型训练

训练并优化了BERT、NLU等小参数模型完成情感识别和意图识别任务。
基于GPT3.5+SFT自动生成万级训练数据，采用P-tuning和LoRA微调技术对LLaMA2-7B模型进行微调。
以LLaMA为基座模型，结合LangChain框架，搭建对用户聊天文本信息进行结构化和分类。
构建并验证了经济数学模型，准确预测了token成本、日活跃用户(DAU)与收益平衡点，为制定最优算法策略提供了数据支持，成功减少了运营成本。

离职原因：公司不重视算法部门，发展受限

---

加州大学洛杉矶分校(美国，洛杉矶)
客座讲师
2021-07 ~ 2023-07

教授融合理论与实践的自然语言处理硕士课程，内容包括但不限于Seq2Seq、RNN、LSTM、Transformer、BERT、GPT。
指导300多名研究生在顶级期刊上发表论文及国际会议上发表演讲，引导学生开展诸多大语言模型(LLM)项目解决AI领域理论和实践挑战。
与AI业界紧密合作，应用LLM将学术研究转化为解决方案，积极参与学术与业界研讨并贡献对新兴技术的见解。

---

诺斯罗普格鲁曼(美国，洛杉矶)
算法专家
2016-08 ~ 2023-07

### 公司介绍

诺思罗普·格鲁曼公司（Northrop Grumman）是世界三大军工生产厂商之一，并列于波音公司和洛克希德-马丁公司，也是世界上最大的雷达制造商和最大的海军船只制造商。

#### 项目背景

负责开发一个高级指挥控制系统，该系统能够理解、分析并执行从指挥室内下达的复杂命令。系统通过自然语言处理技术，将非结构化命令文本转换为结构化指令，并在控制面板上展示相应的回复与效果。

#### 技术成就

1. 命令语句命名实体识别（NER）

模型架构：设计并实现了一个基于BiLSTM+CRF的模型，专门用于从命令语句中抽取关键词和实体，实现文本的结构化处理。
预训练集成：利用Bert预训练模型作为特征提取层，结合BiLSTM捕捉上下文相关的词权重，CRF作为约束层，以优化实体边界的识别。
性能指标：模型在命令语句NER任务上达到了93%的准确率和87%的召回率，显著提高了实体识别的精确度。
实体标注语料库：通过特征提取模型识别新的命名实体，迭代生成高质量的实体标注语料库，用于持续优化NER模型。
知识图谱构建：构建了一个命令知识图谱，使用Neo4j图数据库存储抽取出的三元组（实体-关系-实体），并通过相似度计算链接相关实体，为命令执行提供丰富的上下文信息。

2. 系统回复生成

模型微调：采用LoRA技术对ALBERT模型进行微调，结合Adam优化器，以提高模型对结构化命令的理解能力，并根据不同场景生成恰当的回复。
深度学习网络：使用Tensorflow框架构建了VGG、ResNet、Mobilenet等深度学习分类网络，通过Adam优化器进行模型训练，实现了快速收敛和高性能的模型输出。
自动和人工评估：结合自动评估指标（如BLEU、ROUGE）和人工评估，对生成的回复进行质量控制。
多轮对话处理：实现了多轮对话处理机制，使系统能够在连续的交互中保持对话的一致性和准确性。
持续学习：设计了持续学习机制，使系统能够从新的交互中学习并不断优化其回复生成能力。

3. 地图响应

位置预测模型：运用PyTorch框架，实施RNN、LSTM等深度学习模型，对地图上的目标位置进行预测，实现对动态目标的实时跟踪和定位。
预测精度：通过深度学习模型的集成和优化，提高了地图上目标位置预测的准确性，为指挥决策提供了强有力的技术支持。

---

- 大模型训练与开发：负责利用BERT、GPT、T5等前沿大型预训练模型，开发智能对话系统，实现深层次的自然语言理解、生成和对话管理。
- NLP算法研发领导：主导NLP算法的研发和优化工作，融合深度学习技术（RNN、CNN、LSTM、GRU）和机器学习算法（强化学习RL、决策树DT、SGD、XGBoost等），构建复杂的预测模型，提高推荐系统的准确率。
- 团队管理与敏捷开发：领导算法团队采用敏捷开发模式，快速迭代推荐和预测分析工具。全面管理项目生命周期，采用DevOps实践，促进跨部门协作，确保项目按时按预算交付。
- 构建了一个健壮的DevOps流水线，采用Docker容器技术确保开发和生产环境间的一致性构建。
- 集成了Jenkins进行持续集成，SonarQube进行代码质量分析，以及GCov进行代码覆盖率分析，确保开发过程中维持高标准的代码质量。
- 语音识别技术：运用SpeechBrain框架，识别录音中说话人，完成语音识别、合成、区分、识别、验证，实现95%的召回率和0.9的AUC。
- 弱监督学习方案：提出弱监督co-training方案，以5分钟录音片段为单位进行检测，有效减少正样本噪声，增强模型鲁棒性。
- 特征工程与模型训练：构建n-gram特征，进行特征聚类及场景拆分，使用PMI-n-gram和SimCSE encoder，解决稀疏特征正样本召回问题。
- 模型集成与优化：结合CNN深度模型与n-gram线性模型，实现模型快速上线和高准确率。
- 图像与文本分析：应用Faster R-CNN和Detectron等先进的对象检测算法，结合Visual BERT、ViLBERT、RoBERT、BiLSTM等深度学习模型，实现了对图像和文本内容的多分类意图识别。这些技术的应用显著提升了系统对图像和文本中潜在冒犯性内容的识别精度。
- 深度学习模型搭建：使用Tensorflow框架构建了VGG、ResNet、Mobilenet等深度学习分类网络，并通过Adam优化器进行模型训练。通过严格的模型选择流程，确保了只有高准确率的模型被部署到生产环境中，从而保证了系统的高效性和可靠性。
- 模型融合策略：为了进一步提升检测性能，实施了soft/hard learning方法，整合了不同模型的检测结果。利用PyTorch和Scikit-learn框架，设计了一套模型融合策略，通过综合各个模型的优势，输出了最优的检测结果。
- DevOps实践：构建了一个健壮的DevOps流水线，采用Docker容器技术确保开发和生产环境间的一致性构建。集成了Jenkins进行持续集成，SonarQube进行代码质量分析，以及GCov进行代码覆盖率分析，确保开发过程中维持高标准的代码质量。

---
