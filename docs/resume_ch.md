# 简历

## 教育背景

佐治亚理工学院(美国，佐治亚)
计算机科学(硕士)
2014-09 ~ 2016-06

加州大学洛杉矶分校(美国，洛杉矶)
数学(学士)
2010-09 ~ 2014-06

## 工作经验

---
海纳AI(中国，北京)
NLP算法负责人
2024-03 ~ 至今

- 研发面试测评垂域打模型，对候选人回答进行多维度评估且确保输出结果一致性与合理性。
- 设计并实现重要功能组件，包括但不限于千人千问、面试追问和报告页评估系统以提升面试流程的智能化和互动性。
- 不断研究前沿算法并迭代模型，优化面试系统的各项功能，确保面试测评准确度处于行业领先地位。

---

目的涌现(中国，北京)
AI技术负责人
2023-07 ~ 2024-02

- 从0-1开发通用人工智能(AGI)平台，创建具有高级认知功能的AI Agents，以增强复原复杂社会模拟项目[Stanford Village](https://arxiv.org/abs/2304.03442)
- 独立创建人机对话流程，实现多智能体之间、用户与智能体的自然对话，提升用户体验。通过对话交互，提高用户参与度和平台活跃度。
- 构建经济数学模型预测token成本、DAU与收益平衡点，制定最优算法策略较少运营成本。

---

诺斯罗普格鲁曼(美国，洛杉矶)
算法专家
2016-08 ~ 2023-07

- 领导指挥控制平台(Command&Control)的算法改进工作，应用机器学习、深度学习于复杂的预测模型提高推荐准确率。
- 带领算法团队开发预测分析工具，管理项目全生命周期，促进各部门之间的协作，确保按时并在预算内交付。

---

加州大学洛杉矶分校(美国，洛杉矶)
客座教授
2021-07 ~ 2023-07

- 教授融合理论与实践的自然语言处理(NLP)硕士课程，内容包括但不限于Seq2Seq、RNN、LSTM、Transformer、BERT、GPT。
- 指导300多名研究生在顶级期刊上发表论文及国际会议上发表演讲，引导学生开展诸多大语言模型(LLM)项目解决AI领域理论和实践挑战。
- 与行业紧密合作应用LLM将学术研究转化为解决方案，积极参与学术与行业领域研讨并贡献对新兴技术的见解。

---

## 项目经验

---

AI面试官
算法工程师
2024-03 ~ 至今

- 通过集成FastLLM、vLLM、llama.cpp、TensorRT和Triton服务器等先进加速框架，提升了模型部署和推理速度，减少了延迟。
- 监督了ChatGLM3-6B、Llama2-7B/13B、Qwen-14B和InternLM-7B的精调(SFT)工作，使用了LoRA和QLoRA，并与PPO、DPO和RLHF算法相结合，以对齐模型输出。

---

AGI世界多智能体交互
算法工程师
2023-07 ~ 2024-02

- 开发了一个复杂的多代理框架，通过LangChain和Pinecone向量数据库增强了RAG架构的长期记忆能力，并集成了语音识别(ASR)和文本转语音(TTS)技术，以促进对话交互。
- 整合了最先进的语言模型，如GPT、Whisper、Dalle-3和Gemini，以提升对话交互效果，在验证MVP时显著提高了用户参与度，同时成本极低。
- 对BERT和LLaMA-2等前沿开源大语言模型(LLM)进行了研究，通过优化对话流程和应用LoRA等精调技术，提高了响应质量和准确性。

---

多船位置预测模型
算法工程师
2022-01 ~ 2023-07

- 构建和训练多种机器学习模型，包括监督学习、无监督学习、多项式回归、随机梯度下降(SGD)和随机森林，以预测给定目标的位置。 在测试数据集上实现了85%的平均准确率。
- 使用PyTorch开发和实施高级人工神经网络(ANN)模型，例如循环神经网络(RNN)和长短期记忆(LSTM)网络。与之前使用的模型相比，这些模型将预测精度提高了10%

---

NLP对录音进行转录和语音识别
算法工程师
2021-06 ~ 2022-12

- 开发并实施了强大的语音和自然语言处理(NLP)系统，以应对语音识别、合成、说话人区分/识别/验证方面的关键挑战。 这些系统的性能明显优于现有的行业基准，达到了95%的平均准确率。
- 设计并实施了高效且可扩展的合成处理渠道，以将大量录音转换为准确的转录本。 利用SpeechBrain框架的先进技术来识别录音中每个片段的说话人，从而使处理时间减少20%，整体准确度提高15%

---

使用NLP和计算机视觉技(CV)检测冒犯性内容
算法工程师
2020-01 ~ 2021-05

- 应用FasterRCNN和Detectron等高级对象检测算法来提取图像特征并检测大型图像数据集中的感兴趣对象。与传统的目标检测方法相比，这在准确性和效率方面有了显着提高。
- 利用迁移学习微调和训练多种不同的多模式NLP 转换器架构，包括Visual BERT、ViLBERT、RoBERT等，通过大量的实验和超参数调整，以实现在多个基准数据集上表现最佳的性能。
- 实施了一系列软硬集成学习方法，以结合表现最佳的模型预测，从而显着提高整体准确性。 使用用PyTorch和Scikit-learn，将这些方法无缝集成到现有的模型中，并取得优于最先进技术的结果。

---
